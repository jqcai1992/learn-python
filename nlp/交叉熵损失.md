# 交叉熵损失

## 1. sigmoid和softmax

公式如下
$$
\large sigmoid(x)=1/{1+e^{-x}} \\
\large softmax(z_{i})=e^{z_{i}}/\sum^{K}_{k=1}e^{z_{k}}
$$


## 2. 交叉熵

### a. 多分类的交叉熵

$$
\large Loss=\frac{1}{N}\sum_{i}L_{i}=\frac{1}{N}\sum_{i}-\sum_{c=1}^{M}y_{ic}log(p_{ic})
$$

其中，M是标签个数，求和即为算加权概率；N即为batch size，求几个样本的平均损失



### b. 二分类的交叉熵

$$
\large Loss=\frac{1}{N}\sum_{i}L_{i}=\frac{1}{N}\sum_{i}-[y_{i}log(p_{i})+(1-y_{i})log(1-p_{i})]
$$

yi是0时，取负例的概率对数；yi为1时，取正例的概率对数，为多分类的简化形式。

## 3. 代码

```
import torch


y = torch.Tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]])

y_label = torch.Tensor([[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0]])
soft_y = torch.nn.Softmax(dim=-1)(y)
#tensor([[0.0900, 0.0900, 0.0900],
#        [0.2447, 0.2447, 0.2447],
#        [0.6652, 0.6652, 0.6652]])
loss1 = -torch.mean(torch.sum(y_label*torch.log(soft_y),dim=0))
#tensor(0.4076)
y_label2 = torch.LongTensor([2, 2, 2])
loss2 = torch.nn.CrossEntropyLoss()(y, y_label2)
#tensor(0.4076)


```

